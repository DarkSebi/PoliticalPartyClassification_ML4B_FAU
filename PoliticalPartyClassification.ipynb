{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "!pip install -q -U json-lines\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import json_lines\n",
    "from tqdm.notebook import tqdm\n",
    "from pprint import pprint\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read party, tweets, mentions and hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5434548424ed442584f13bbfafa2657e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=727.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of error-requests: 0\n",
      "Read 1806094 Tweets sucussfully.\n",
      "mentions 0\n",
      "hashtags 0\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "#funktioniert: get Party name + Tweets\n",
    "files123 = glob.glob('./data/*.jl')\n",
    "\n",
    "partyname_n_tweets = []\n",
    "partyname_mentions = []\n",
    "partyname_hash = []\n",
    "error = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file in tqdm(files123): # responsible for visual output\n",
    "\n",
    "    #if(i == 300):\n",
    "        #continue\n",
    "    #i += 1\n",
    "    \n",
    "    with json_lines.open(file) as f:\n",
    "        for item in f:\n",
    "            try:\n",
    "                if item['response']['meta']['result_count'] == 0: \n",
    "                    continue\n",
    "                elif item['account_data']['Partei'] == \"Fraktionslos\":\n",
    "                    continue\n",
    "                if item['http_status'] == 200:\n",
    "                    \n",
    "                    #get party name\n",
    "                    partei = item[\"account_data\"][\"Partei\"]\n",
    "                    \n",
    "                    #get all tweets per person\n",
    "                    tweets_array = item[\"response\"][\"data\"]\n",
    "                    \n",
    "                    for tweet in tweets_array: #range(0,len(tweets_array)-1):\n",
    "                       # print(type(tweet))\n",
    "                        if(\"referenced_tweets\" in tweet): # tweet is a dictionary\n",
    "                            tweet_type = tweet[\"referenced_tweets\"]\n",
    "                            if(tweet_type[0][\"type\"] == \"retweeted\"):\n",
    "                                continue\n",
    "                        partyname_n_tweets.append({\"Partei\": partei, \"Tweet\": tweet[\"text\"]}) # hashtags, mentions in eigene Arrays\n",
    "                        \n",
    "                        #if(\"entities\" in tweet): \n",
    "                            #entities = tweet[\"entities\"]\n",
    "                            #if(\"mentions\" in entities):\n",
    "                                #mentions = entities[\"mentions\"]\n",
    "                                #for ment in mentions:\n",
    "                                        #mention = ment[\"username\"]\n",
    "                                        #partyname_mentions.append({\"Partei\": partei, \"mentions\": mention})\n",
    "                            #if(\"hashtags\" in entities):\n",
    "                                #hashtags = entities[\"hashtags\"]\n",
    "                                #for tag in hashtags:\n",
    "                                        #hashtag = tag[\"tag\"]\n",
    "                                        #partyname_hash.append({\"Partei\": partei, \"hashtags\": hashtag})\n",
    "                                        \n",
    "                else:\n",
    "                    error.append(item)\n",
    "                    print(\"yikes\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                raise e\n",
    "                           \n",
    "print(\"Number of error-requests:\", len(error))\n",
    "print(\"Read\", len(partyname_n_tweets), \"Tweets sucussfully.\")\n",
    "print(\"mentions\", len(partyname_mentions))\n",
    "print(\"hashtags\", len(partyname_hash))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tweets insgesamt\", len(partyname_n_tweets))\n",
    "partyname_n_tweets_df = pd.DataFrame(partyname_n_tweets)\n",
    "partyname_n_tweets_df[\"Partei\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "##Versuch auf uneleganter Weise: ----\n",
    "files123 = glob.glob('./data/*.jl')\n",
    "\n",
    "partyname_n_tweets_without_retweets = []\n",
    "error = []\n",
    "\n",
    "for file in tqdm(files123): # responsible for visual output\n",
    "    with json_lines.open(file) as f:\n",
    "        for item in f:\n",
    "            try:\n",
    "                if item['response']['meta']['result_count'] == 0: \n",
    "                    continue\n",
    "                elif item['account_data']['Partei'] == \"Fraktionslos\":\n",
    "                    continue\n",
    "                if item['http_status'] == 200:\n",
    "                    \n",
    "                    #get party name\n",
    "                    partei = item[\"account_data\"][\"Partei\"]\n",
    "                    \n",
    "                    #get all tweets per person\n",
    "                    tweets_array = item[\"response\"][\"data\"]\n",
    "                    for tweet in tweets_array:\n",
    "                      \n",
    "                        if tweet[\"text\"].startswith(\"RT @\"):\n",
    "                            continue\n",
    "                        \n",
    "                        partyname_n_tweets_without_retweets.append({\"Partei\": partei, \"Tweet\": tweet[\"text\"]})\n",
    "                else:\n",
    "                    error.append(item)\n",
    "                    print(\"yikes\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                raise e\n",
    "                           \n",
    "print(\"Number of error-requests:\", len(error))\n",
    "print(\"Read\", len(partyname_n_tweets_without_retweets), \"Tweets sucussfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3_1\n",
    "text_df = pd.DataFrame(partyname_n_tweets_without_retweets)\n",
    "print(text_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failed H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3_2\n",
    "#!apt -get install default-jre\n",
    "#!java -version\n",
    "!pip install h2o\n",
    "#!pip install emojis\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3_3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import emojis\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) Client VM (build 25.281-b09, mixed mode)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Runjie\\anaconda3\\lib\\site-packages\\h2o\\backend\\server.py:385: UserWarning:   You have a 32-bit version of Java. H2O works best with 64-bit Java.\n",
      "  Please download the latest 64-bit Java SE JDK from Oracle.\n",
      "\n",
      "  warn(\"  You have a 32-bit version of Java. H2O works best with 64-bit Java.\\n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Starting server from C:\\Users\\Runjie\\anaconda3\\Lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\Runjie\\AppData\\Local\\Temp\\tmp_vkj5c3s\n",
      "  JVM stdout: C:\\Users\\Runjie\\AppData\\Local\\Temp\\tmp_vkj5c3s\\h2o_Runjie_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\Runjie\\AppData\\Local\\Temp\\tmp_vkj5c3s\\h2o_Runjie_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Berlin</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.32.1.2</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>28 days, 22 hours and 16 minutes </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_Runjie_71li82</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>247.5 Mb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.8.5 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------------\n",
       "H2O_cluster_uptime:         02 secs\n",
       "H2O_cluster_timezone:       Europe/Berlin\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.32.1.2\n",
       "H2O_cluster_version_age:    28 days, 22 hours and 16 minutes\n",
       "H2O_cluster_name:           H2O_from_python_Runjie_71li82\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    247.5 Mb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         accepting new members, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.8.5 final\n",
       "--------------------------  ---------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Runjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B√ºndnis 90/Die Gr√ºnen    33707\n",
       "SPD                      27794\n",
       "CDU                      26134\n",
       "Die Linke                25681\n",
       "FDP                      14426\n",
       "AfD                       7902\n",
       "CSU                       5497\n",
       "Name: Partei, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnt_df = pd.DataFrame(partyname_n_tweets)\n",
    "pnt_df = pnt_df.sample(n=141141, random_state = 42)\n",
    "pnt_df[\"Partei\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Partei</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>751143</th>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "      <td>ist nat√ºrlich dabei! Abstimmen f√ºr volle Gleic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517497</th>\n",
       "      <td>FDP</td>\n",
       "      <td>@Energisch_ warum nicht! #Luxemburg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552452</th>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "      <td>Schwarz-Gelb beschlie√üt gegen Opppsition Haush...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138650</th>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "      <td>Sonder-Fraktionssitzung zu #Griechenland</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750667</th>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "      <td>#Gr√ºne NDS: Superergebnisse in Uni-St√§dten, gr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740624</th>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "      <td>@WollErnst Sie m√ºssen sich schon sich den Inha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558568</th>\n",
       "      <td>CSU</td>\n",
       "      <td>Wallersdorfer Heimaterde in Berlin. \\nEine sch...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487369</th>\n",
       "      <td>SPD</td>\n",
       "      <td>Da kann der #Sarrazin bis zum Papst gehen. Ras...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700942</th>\n",
       "      <td>CSU</td>\n",
       "      <td>@verenahubertz @fdpbt @EUTheurer Es kann nach ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493234</th>\n",
       "      <td>SPD</td>\n",
       "      <td>Jubilarehrung \"auf Schwerin\".</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141141 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Partei  \\\n",
       "751143   B√ºndnis 90/Die Gr√ºnen   \n",
       "517497                     FDP   \n",
       "1552452  B√ºndnis 90/Die Gr√ºnen   \n",
       "138650   B√ºndnis 90/Die Gr√ºnen   \n",
       "750667   B√ºndnis 90/Die Gr√ºnen   \n",
       "...                        ...   \n",
       "1740624  B√ºndnis 90/Die Gr√ºnen   \n",
       "558568                     CSU   \n",
       "487369                     SPD   \n",
       "700942                     CSU   \n",
       "493234                     SPD   \n",
       "\n",
       "                                                     Tweet  target  \n",
       "751143   ist nat√ºrlich dabei! Abstimmen f√ºr volle Gleic...       0  \n",
       "517497                 @Energisch_ warum nicht! #Luxemburg       4  \n",
       "1552452  Schwarz-Gelb beschlie√üt gegen Opppsition Haush...       0  \n",
       "138650            Sonder-Fraktionssitzung zu #Griechenland       0  \n",
       "750667   #Gr√ºne NDS: Superergebnisse in Uni-St√§dten, gr...       0  \n",
       "...                                                    ...     ...  \n",
       "1740624  @WollErnst Sie m√ºssen sich schon sich den Inha...       0  \n",
       "558568   Wallersdorfer Heimaterde in Berlin. \\nEine sch...       6  \n",
       "487369   Da kann der #Sarrazin bis zum Papst gehen. Ras...       1  \n",
       "700942   @verenahubertz @fdpbt @EUTheurer Es kann nach ...       6  \n",
       "493234                       Jubilarehrung \"auf Schwerin\".       1  \n",
       "\n",
       "[141141 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target={\"B√ºndnis 90/Die Gr√ºnen\":0, \"SPD\":1, \"CDU\":2, \"Die Linke\":3, \"FDP\":4, \"AfD\":5, \"CSU\":6}\n",
    "pnt_df[\"target\"]=pnt_df[\"Partei\"].map(target)\n",
    "pnt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(pnt_df, test_size=0.4, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Runjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer(\"german\")\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words=set(nltk.corpus.stopwords.words(\"german\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'derselbe', 'w√ºrden', 'dazu', 'deiner', 'anderes', 'doch', 'welches', 'dieselbe', 'jene', 'meines', 'viel', 'so', 'ihnen', 'was', 'auch', 'wenn', 'zwar', 'welche', 'wir', 'an', 'anderr', 'andere', 'meinem', 'zum', 'sondern', 'eine', 'waren', 'anders', 'hin', 'sind', 'man', 'deinen', 'werde', 'einen', 'war', 'diesem', 'unter', 'derselben', 'sein', 'bis', 'jeden', 'am', 'manches', 'k√∂nnen', 'seinem', 'damit', 'das', 'oder', 'einigem', 'musste', 'dasselbe', 'ihres', 'des', 'meinen', 'deinem', 'jede', 'haben', 'und', 'ins', 'mich', 'durch', 'eures', 'einig', 'die', 'vor', 'eurer', 'weiter', 'ihre', 'hier', 'bin', 'dann', 'keines', 'seines', 'dein', 'manchen', 'nach', 'zu', 'keinem', 'manche', 'ihr', 'einiger', 'derer', 'in', 'im', 'solches', 'uns', 'wieder', 'jetzt', 'indem', 'soll', 'aber', 'zur', 'allen', 'keinen', 'unseren', 'solchen', 'eurem', 'nur', 'solchem', 'aus', 'dieselben', 'deines', 'welcher', 'anderem', 'da√ü', 'f√ºr', 'hab', 'hat', 'dies', 'habe', 'wird', 'mancher', 'dieser', 'seiner', 'ich', 'gegen', 'wollen', 'zwischen', 'um', 'meine', 'von', 'auf', 'als', 'einige', 'aller', 'muss', 'euch', 'einmal', 'euren', 'unseres', 'ihm', 'seine', 'allem', 'jenen', 'anderer', 'der', 'nun', 'meiner', 'k√∂nnte', 'jenem', 'noch', 'manchem', 'diese', 'alles', 'jener', 'deine', 'sehr', 'hatten', 'jedem', 'ihn', 'solche', 'den', 'dir', 'sonst', 'dessen', 'hatte', 'mein', 'desselben', 'keine', 'wirst', 'mit', 'jenes', 'werden', 'ohne', 'bist', 'etwas', 'dem', 'einem', 'denn', 'ein', 'sie', 'da', 'unsere', 'ihrer', 'dass', 'einiges', 'einer', 'nicht', 'dieses', 'gewesen', 'eines', 'nichts', 'eure', 'keiner', 'seinen', 'vom', 'weg', 'weil', 'wo', 'selbst', 'diesen', 'alle', 'hinter', 'du', 'kann', 'warst', 'w√§hrend', 'will', 'welchen', '√ºber', 'sollte', 'unserem', 'demselben', 'solcher', 'wie', 'also', 'ander', 'bei', 'dich', 'jeder', 'anderen', 'euer', 'denselben', 'anderm', 'er', 'w√ºrde', 'ist', 'andern', 'einigen', 'machen', 'sich', 'welchem', 'wollte', 'dort', 'unser', 'ob', 'es', 'jedes', 'mir', 'kein', 'ihrem', 'ihren'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = [word for word in nltk.word_tokenize(text)] #if (len(word) > 3 and len(word.strip(\"Xx/\")) > 2 and len(re.sub(\"\\d+\", \"\", word.strip(\"Xx/\")))]\n",
    "    tokens = map(str.lower, tokens)\n",
    "    tokens = [word.strip(\" #|‚Ç¨!?,.:;'`‚Äò‚Äô‚Äú‚Äù‚Äû@[]()=*+-‚Äì%/0123456789& \") for word in tokens]\n",
    "    tokens = [word for word in tokens if not word in emoji.UNICODE_EMOJI]\n",
    "    ##tokens = [word.strip() for word in tokens]\n",
    "    stems = [stemmer.stem(item) for item in tokens if (item not in stop_words)]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tf = TfidfVectorizer(tokenizer=tokenize, stop_words=None, max_df=0.75, max_features=1000, lowercase=False, ngram_range=(1,2))\n",
    "train_vectors=vectorizer_tf.fit_transform(X_train.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33343994, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.11408344, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.32733697, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.52161588, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.31981752, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors.A[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' a',\n",
       " ' ab',\n",
       " ' afd',\n",
       " ' afdimbundestag',\n",
       " ' akk',\n",
       " ' amp',\n",
       " ' arbeit',\n",
       " ' bay',\n",
       " ' beim',\n",
       " ' berlin',\n",
       " ' bess',\n",
       " ' bild',\n",
       " ' bildung',\n",
       " ' bitt',\n",
       " ' bleibt',\n",
       " ' brauch',\n",
       " ' btw',\n",
       " ' bundesregier',\n",
       " ' bundestag',\n",
       " ' bundeswehr',\n",
       " ' c_lindn',\n",
       " ' cdu',\n",
       " ' cducsubt',\n",
       " ' corona',\n",
       " ' csu',\n",
       " ' d',\n",
       " ' dabei',\n",
       " ' dafur',\n",
       " ' dank',\n",
       " ' darf',\n",
       " ' demokrati',\n",
       " ' deshalb',\n",
       " ' deutsch',\n",
       " ' deutschland',\n",
       " ' die_gru',\n",
       " ' dielink',\n",
       " ' doroba',\n",
       " ' einfach',\n",
       " ' endlich',\n",
       " ' energiew',\n",
       " ' erst',\n",
       " ' eu',\n",
       " ' euro',\n",
       " ' europa',\n",
       " ' faznet',\n",
       " ' fb',\n",
       " ' fdp',\n",
       " ' fdpbt',\n",
       " ' find',\n",
       " ' fluchtling',\n",
       " ' frag',\n",
       " ' frau',\n",
       " ' freiheit',\n",
       " ' freu',\n",
       " ' ganz',\n",
       " ' geht',\n",
       " ' gemeinsam',\n",
       " ' genau',\n",
       " ' gerad',\n",
       " ' gibt',\n",
       " ' gleich',\n",
       " ' groko',\n",
       " ' gross',\n",
       " ' gruenebundestag',\n",
       " ' grun',\n",
       " ' gut',\n",
       " ' halt',\n",
       " ' hatt',\n",
       " ' heikomaas',\n",
       " ' herzlich',\n",
       " ' heut',\n",
       " ' http',\n",
       " ' https',\n",
       " ' imm',\n",
       " ' inn',\n",
       " ' ja',\n",
       " ' jahr',\n",
       " ' jensspahn',\n",
       " ' kahr',\n",
       " ' kind',\n",
       " ' klar',\n",
       " ' klimaschutz',\n",
       " ' komm',\n",
       " ' kommt',\n",
       " ' konstantinnotz',\n",
       " ' land',\n",
       " ' lang',\n",
       " ' larsklingbeil',\n",
       " ' leid',\n",
       " ' letzt',\n",
       " ' lieb',\n",
       " ' link',\n",
       " ' linksfraktion',\n",
       " ' lt',\n",
       " ' macht',\n",
       " ' mal',\n",
       " ' mehr',\n",
       " ' mensch',\n",
       " ' merkel',\n",
       " ' mio',\n",
       " ' morg',\n",
       " ' mrd',\n",
       " ' muss',\n",
       " ' natur',\n",
       " ' nein',\n",
       " ' neu',\n",
       " ' noafd',\n",
       " ' nrw',\n",
       " ' olafscholz',\n",
       " ' peteraltmai',\n",
       " ' polit',\n",
       " ' prozent',\n",
       " ' r2g',\n",
       " ' recht',\n",
       " ' red',\n",
       " ' richtig',\n",
       " ' rt',\n",
       " ' s',\n",
       " ' sag',\n",
       " ' sagt',\n",
       " ' schon',\n",
       " ' schul',\n",
       " ' seehof',\n",
       " ' seh',\n",
       " ' seit',\n",
       " ' sich',\n",
       " ' sozial',\n",
       " ' spd',\n",
       " ' spdbt',\n",
       " ' spdde',\n",
       " ' spiegelonlin',\n",
       " ' stark',\n",
       " ' statt',\n",
       " ' sz',\n",
       " ' tag',\n",
       " ' tagesspiegel',\n",
       " ' tb',\n",
       " ' thema',\n",
       " ' thuring',\n",
       " ' toll',\n",
       " ' trump',\n",
       " ' turkei',\n",
       " ' uhr',\n",
       " ' union',\n",
       " ' usa',\n",
       " ' viel',\n",
       " ' vielleicht',\n",
       " ' wahl',\n",
       " ' war',\n",
       " ' warum',\n",
       " ' weit',\n",
       " ' welt',\n",
       " ' wenig',\n",
       " ' wer',\n",
       " ' wichtig',\n",
       " ' wunsch',\n",
       " ' wurd',\n",
       " ' zdf',\n",
       " ' zeit',\n",
       " ' üòâ',\n",
       " 'a',\n",
       " 'ab',\n",
       " 'ab ',\n",
       " 'abend',\n",
       " 'abend ',\n",
       " 'abgeordnet',\n",
       " 'abschaff',\n",
       " 'absolut',\n",
       " 'abstimm',\n",
       " 'ach',\n",
       " 'afd',\n",
       " 'afd ',\n",
       " 'afd https',\n",
       " 'afdimbundestag',\n",
       " 'akk',\n",
       " 'aktion',\n",
       " 'aktuell',\n",
       " 'all',\n",
       " 'allein',\n",
       " 'allerding',\n",
       " 'alt',\n",
       " 'alternativ',\n",
       " 'amp',\n",
       " 'amp ',\n",
       " 'and',\n",
       " 'and ',\n",
       " 'anfang',\n",
       " 'angst',\n",
       " 'anhor',\n",
       " 'antrag',\n",
       " 'antwort',\n",
       " 'antwort ',\n",
       " 'arbeit',\n",
       " 'arbeit ',\n",
       " 'are',\n",
       " 'argument',\n",
       " 'art',\n",
       " 'artikel',\n",
       " 'artikel ',\n",
       " 'at',\n",
       " 'aufgab',\n",
       " 'aufklar',\n",
       " 'auss',\n",
       " 'aussag',\n",
       " 'ausschuss',\n",
       " 'austausch',\n",
       " 'b',\n",
       " 'bahn',\n",
       " 'bald',\n",
       " 'bau',\n",
       " 'bay',\n",
       " 'bay ',\n",
       " 'bdk',\n",
       " 'be',\n",
       " 'beginnt',\n",
       " 'beid',\n",
       " 'beim',\n",
       " 'beim ',\n",
       " 'beispiel',\n",
       " 'beitrag',\n",
       " 'bekampf',\n",
       " 'bekannt',\n",
       " 'bekomm',\n",
       " 'bekomm ',\n",
       " 'berat',\n",
       " 'bereit',\n",
       " 'bericht',\n",
       " 'berichtet',\n",
       " 'berlin',\n",
       " 'berlin ',\n",
       " 'beschloss',\n",
       " 'besond',\n",
       " 'bess',\n",
       " 'bess ',\n",
       " 'best',\n",
       " 'bestimmt',\n",
       " 'besuch',\n",
       " 'besucht',\n",
       " 'bild',\n",
       " 'bild ',\n",
       " 'bildung',\n",
       " 'bildung ',\n",
       " 'bish',\n",
       " 'bitt',\n",
       " 'bitt ',\n",
       " 'bleib',\n",
       " 'bleib ',\n",
       " 'bleibt',\n",
       " 'blick',\n",
       " 'bonn',\n",
       " 'botschaft',\n",
       " 'brandenburg',\n",
       " 'brauch',\n",
       " 'brauch ',\n",
       " 'braucht',\n",
       " 'brexit',\n",
       " 'bring',\n",
       " 'bringt',\n",
       " 'btw',\n",
       " 'btw ',\n",
       " 'bund',\n",
       " 'bundesregier',\n",
       " 'bundesregier ',\n",
       " 'bundestag',\n",
       " 'bundestag ',\n",
       " 'bundeswehr',\n",
       " 'burg',\n",
       " 'burg ',\n",
       " 'c_lindn',\n",
       " 'cdu',\n",
       " 'cdu ',\n",
       " 'cducsubt',\n",
       " 'cducsubt ',\n",
       " 'chanc',\n",
       " 'cl',\n",
       " 'co',\n",
       " 'corona',\n",
       " 'csu',\n",
       " 'csu ',\n",
       " 'd',\n",
       " 'd ',\n",
       " 'dabei',\n",
       " 'dabei ',\n",
       " 'dafur',\n",
       " 'dafur ',\n",
       " 'dageg',\n",
       " 'dah',\n",
       " 'danach',\n",
       " 'dank',\n",
       " 'dank ',\n",
       " 'daran',\n",
       " 'darauf',\n",
       " 'darf',\n",
       " 'darub',\n",
       " 'darum',\n",
       " 'davon',\n",
       " 'de',\n",
       " 'debatt',\n",
       " 'debatt ',\n",
       " 'demokrat',\n",
       " 'demokrati',\n",
       " 'demokrati ',\n",
       " 'den',\n",
       " 'denk',\n",
       " 'deshalb',\n",
       " 'deutlich',\n",
       " 'deutsch',\n",
       " 'deutsch ',\n",
       " 'deutsch bundestag',\n",
       " 'deutschland',\n",
       " 'deutschland ',\n",
       " 'die_gru',\n",
       " 'die_gru ',\n",
       " 'dielink',\n",
       " 'dielink ',\n",
       " 'digital',\n",
       " 'digitalisier',\n",
       " 'direkt',\n",
       " 'diskussion',\n",
       " 'diskussion ',\n",
       " 'diskuti',\n",
       " 'diskutiert',\n",
       " 'doroba',\n",
       " 'dr',\n",
       " 'dran',\n",
       " 'drei',\n",
       " 'dringend',\n",
       " 'druck',\n",
       " 'durf',\n",
       " 'eben',\n",
       " 'echt',\n",
       " 'egal',\n",
       " 'eher',\n",
       " 'eig',\n",
       " 'eigent',\n",
       " 'einfach',\n",
       " 'einfach ',\n",
       " 'einlad',\n",
       " 'einsatz',\n",
       " 'einzig',\n",
       " 'end',\n",
       " 'end ',\n",
       " 'endlich',\n",
       " 'energiew',\n",
       " 'engagement',\n",
       " 'entscheid',\n",
       " 'entscheid ',\n",
       " 'entwickl',\n",
       " 'erdogan',\n",
       " 'erfolg',\n",
       " 'erfolg ',\n",
       " 'erfolgreich',\n",
       " 'ergebnis',\n",
       " 'erhalt',\n",
       " 'erklar',\n",
       " 'erklart',\n",
       " 'ernst',\n",
       " 'erst',\n",
       " 'erst ',\n",
       " 'erwart',\n",
       " 'ess',\n",
       " 'eu',\n",
       " 'eu ',\n",
       " 'euro',\n",
       " 'europa',\n",
       " 'europa ',\n",
       " 'f',\n",
       " 'facebook',\n",
       " 'facebook gepostet',\n",
       " 'fair',\n",
       " 'fakt',\n",
       " 'fall',\n",
       " 'fall ',\n",
       " 'falsch',\n",
       " 'falsch ',\n",
       " 'famili',\n",
       " 'fast',\n",
       " 'faznet',\n",
       " 'fb',\n",
       " 'fdp',\n",
       " 'fdp ',\n",
       " 'fdpbt',\n",
       " 'fdpbt ',\n",
       " 'fehl',\n",
       " 'fehlt',\n",
       " 'fest',\n",
       " 'find',\n",
       " 'find ',\n",
       " 'findet',\n",
       " 'fluchtling',\n",
       " 'folg',\n",
       " 'for',\n",
       " 'ford',\n",
       " 'ford ',\n",
       " 'forder',\n",
       " 'fordert',\n",
       " 'foto',\n",
       " 'frag',\n",
       " 'frag ',\n",
       " 'fraktion',\n",
       " 'frankfurt',\n",
       " 'frau',\n",
       " 'frau ',\n",
       " 'frei',\n",
       " 'freiheit',\n",
       " 'freiheit ',\n",
       " 'freu',\n",
       " 'freu ',\n",
       " 'freund',\n",
       " 'freut',\n",
       " 'fried',\n",
       " 'fruh',\n",
       " 'fuhr',\n",
       " 'fuhrt',\n",
       " 'gab',\n",
       " 'gabriel',\n",
       " 'ganz',\n",
       " 'gar',\n",
       " 'gar ',\n",
       " 'gast',\n",
       " 'geb',\n",
       " 'geb ',\n",
       " 'gedank',\n",
       " 'gefahr',\n",
       " 'gegenub',\n",
       " 'geh',\n",
       " 'geh ',\n",
       " 'gehort',\n",
       " 'geht',\n",
       " 'geht ',\n",
       " 'geht s',\n",
       " 'geld',\n",
       " 'gemacht',\n",
       " 'gemacht ',\n",
       " 'gemeinsam',\n",
       " 'genau',\n",
       " 'genau ',\n",
       " 'genug',\n",
       " 'gepostet',\n",
       " 'gerad',\n",
       " 'gerad ',\n",
       " 'gerecht',\n",
       " 'gern',\n",
       " 'gern ',\n",
       " 'gesagt',\n",
       " 'gesagt ',\n",
       " 'geschicht',\n",
       " 'gesellschaft',\n",
       " 'gesetz',\n",
       " 'gespannt',\n",
       " 'gesprach',\n",
       " 'gesprach ',\n",
       " 'gest',\n",
       " 'gesund',\n",
       " 'gewahlt',\n",
       " 'gewahlt ',\n",
       " 'gewalt',\n",
       " 'gewinn',\n",
       " 'gg',\n",
       " 'gibt',\n",
       " 'gibt ',\n",
       " 'gilt',\n",
       " 'ging',\n",
       " 'glaub',\n",
       " 'glaub ',\n",
       " 'gleich',\n",
       " 'gleich ',\n",
       " 'gluckwunsch',\n",
       " 'gluckwunsch ',\n",
       " 'grenz',\n",
       " 'griechenland',\n",
       " 'groko',\n",
       " 'groko ',\n",
       " 'gross',\n",
       " 'grossart',\n",
       " 'grosst',\n",
       " 'gruenebundestag',\n",
       " 'gruenebundestag ',\n",
       " 'grun',\n",
       " 'grun ',\n",
       " 'grund',\n",
       " 'grundgesetz',\n",
       " 'gruss',\n",
       " 'gut',\n",
       " 'gut ',\n",
       " 'gut morg',\n",
       " 'h',\n",
       " 'halt',\n",
       " 'halt ',\n",
       " 'hamburg',\n",
       " 'hand',\n",
       " 'handeln',\n",
       " 'hart',\n",
       " 'hass',\n",
       " 'hast',\n",
       " 'hatt',\n",
       " 'hatt ',\n",
       " 'haus',\n",
       " 'haushalt',\n",
       " 'heikomaas',\n",
       " 'heimat',\n",
       " 'heisst',\n",
       " 'helf',\n",
       " 'her',\n",
       " 'herr',\n",
       " 'herzlich',\n",
       " 'herzlich gluckwunsch',\n",
       " 'hess',\n",
       " 'heut',\n",
       " 'heut ',\n",
       " 'heut abend',\n",
       " 'heutig',\n",
       " 'hilf',\n",
       " 'hilft',\n",
       " 'hinweis',\n",
       " 'hoch',\n",
       " 'hoff',\n",
       " 'hoff ',\n",
       " 'hoh',\n",
       " 'hor',\n",
       " 'http',\n",
       " 'http ',\n",
       " 'https',\n",
       " 'https ',\n",
       " 'i',\n",
       " 'ide',\n",
       " 'ide ',\n",
       " 'imm',\n",
       " 'imm ',\n",
       " 'inhalt',\n",
       " 'initiativ',\n",
       " 'inn',\n",
       " 'inn ',\n",
       " 'interess',\n",
       " 'interessant',\n",
       " 'international',\n",
       " 'internet',\n",
       " 'interview',\n",
       " 'interview ',\n",
       " 'is',\n",
       " 'israel',\n",
       " 'ja',\n",
       " 'ja ',\n",
       " 'jahr',\n",
       " 'jahr ',\n",
       " 'je',\n",
       " 'jemand',\n",
       " 'jensspahn',\n",
       " 'journalist',\n",
       " 'jugend',\n",
       " 'jung',\n",
       " 'kahr',\n",
       " 'kampf',\n",
       " 'kanzlerin',\n",
       " 'kaum',\n",
       " 'kenn',\n",
       " 'kind',\n",
       " 'kind ',\n",
       " 'klar',\n",
       " 'klar ',\n",
       " 'klass',\n",
       " 'klein',\n",
       " 'klimaschutz',\n",
       " 'koalition',\n",
       " 'kolleg',\n",
       " 'kolleg ',\n",
       " 'koln',\n",
       " 'komm',\n",
       " 'komm ',\n",
       " 'kommentar',\n",
       " 'kommt',\n",
       " 'kommt ',\n",
       " 'kommun',\n",
       " 'konkret',\n",
       " 'konnt',\n",
       " 'konstantinnotz',\n",
       " 'kost',\n",
       " 'kraft',\n",
       " 'krieg',\n",
       " 'kris',\n",
       " 'kritik',\n",
       " 'kultur',\n",
       " 'kurz',\n",
       " 'lag',\n",
       " 'land',\n",
       " 'land ',\n",
       " 'landtag',\n",
       " 'lang',\n",
       " 'lang ',\n",
       " 'larsklingbeil',\n",
       " 'lass',\n",
       " 'lass ',\n",
       " 'lasst',\n",
       " 'lauft',\n",
       " 'laut',\n",
       " 'leb',\n",
       " 'leb ',\n",
       " 'leid',\n",
       " 'leid ',\n",
       " 'leipzig',\n",
       " 'les',\n",
       " 'les ',\n",
       " 'lesenswert',\n",
       " 'letzt',\n",
       " 'leut',\n",
       " 'liberal',\n",
       " 'lieb',\n",
       " 'lieb ',\n",
       " 'liegt',\n",
       " 'link',\n",
       " 'link ',\n",
       " 'linksfraktion',\n",
       " 'linksfraktion ',\n",
       " 'liv',\n",
       " 'los',\n",
       " 'los ',\n",
       " 'losung',\n",
       " 'lt',\n",
       " 'lt ',\n",
       " 'm',\n",
       " 'mach',\n",
       " 'macht',\n",
       " 'macht ',\n",
       " 'mai',\n",
       " 'mal',\n",
       " 'mal ',\n",
       " 'mann',\n",
       " 'mann ',\n",
       " 'massiv',\n",
       " 'massnahm',\n",
       " 'mdb',\n",
       " 'medi',\n",
       " 'mehr',\n",
       " 'mehr ',\n",
       " 'mehrheit',\n",
       " 'meint',\n",
       " 'meinung',\n",
       " 'meist',\n",
       " 'mensch',\n",
       " 'mensch ',\n",
       " 'menschenrecht',\n",
       " 'merkel',\n",
       " 'merkel ',\n",
       " 'million',\n",
       " 'mindestlohn',\n",
       " 'minist',\n",
       " 'mio',\n",
       " 'mitglied',\n",
       " 'mitt',\n",
       " 'mocht',\n",
       " 'moglich',\n",
       " 'moglich ',\n",
       " 'monat',\n",
       " 'morg',\n",
       " 'morg ',\n",
       " 'mrd',\n",
       " 'munch',\n",
       " 'muss',\n",
       " 'muss ',\n",
       " 'musst',\n",
       " 'my',\n",
       " 'na',\n",
       " 'nach',\n",
       " 'nachhalt',\n",
       " 'nachricht',\n",
       " 'nacht',\n",
       " 'natur',\n",
       " 'nazis',\n",
       " 'ne',\n",
       " 'nehm',\n",
       " 'nehm ',\n",
       " 'nein',\n",
       " 'nein ',\n",
       " 'neu',\n",
       " 'neu ',\n",
       " 'nie',\n",
       " 'niemand',\n",
       " 'nimmt',\n",
       " 'nix',\n",
       " 'no',\n",
       " 'noafd',\n",
       " 'nochmal',\n",
       " 'not',\n",
       " 'notig',\n",
       " 'notwend',\n",
       " 'nrw',\n",
       " 'nsu',\n",
       " 'nutz',\n",
       " 'of',\n",
       " 'off',\n",
       " 'offenbar',\n",
       " 'offent',\n",
       " 'oft',\n",
       " 'oh',\n",
       " 'olafscholz',\n",
       " 'on',\n",
       " 'onlin',\n",
       " 'opf',\n",
       " 'opposition',\n",
       " 'ort',\n",
       " 'paar',\n",
       " 'parlament',\n",
       " 'parlamentar',\n",
       " 'partei',\n",
       " 'partei ',\n",
       " 'parteitag',\n",
       " 'passt',\n",
       " 'peinlich',\n",
       " 'per',\n",
       " 'person',\n",
       " 'pet',\n",
       " 'peteraltmai',\n",
       " 'pfleg',\n",
       " 'pirat',\n",
       " 'plan',\n",
       " 'platz',\n",
       " 'platz ',\n",
       " 'plenum',\n",
       " 'polit',\n",
       " 'polit ',\n",
       " 'polizei',\n",
       " 'position',\n",
       " 'prasident',\n",
       " 'probl',\n",
       " 'probl ',\n",
       " 'problem',\n",
       " 'programm',\n",
       " 'projekt',\n",
       " 'prozent',\n",
       " 'punkt',\n",
       " 'r2g',\n",
       " 'rassismus',\n",
       " 'rassist',\n",
       " 'raum',\n",
       " 'raus',\n",
       " 'recht',\n",
       " 'recht ',\n",
       " 'rechtsstaat',\n",
       " 'red',\n",
       " 'red ',\n",
       " 'reform',\n",
       " 'regier',\n",
       " 'region',\n",
       " 'reich',\n",
       " 'reicht',\n",
       " 'rein',\n",
       " 'rent',\n",
       " 'respekt',\n",
       " 'richtig',\n",
       " 'richtig ',\n",
       " 'rot',\n",
       " 'rt',\n",
       " 'rt ',\n",
       " 'rund',\n",
       " 'russland',\n",
       " 's',\n",
       " 's ',\n",
       " 'sach',\n",
       " 'sach ',\n",
       " 'sachs',\n",
       " 'sag',\n",
       " 'sag ',\n",
       " 'sagt',\n",
       " 'sagt ',\n",
       " 'schad',\n",
       " 'schad ',\n",
       " 'schaff',\n",
       " 'schaff ',\n",
       " 'schau',\n",
       " 'scheint',\n",
       " 'schlecht',\n",
       " 'schlimm',\n",
       " 'schnell',\n",
       " 'schon',\n",
       " 'schon ',\n",
       " 'schritt',\n",
       " 'schul',\n",
       " 'schul ',\n",
       " 'schuld',\n",
       " 'schutz',\n",
       " 'schutz ',\n",
       " 'schwer',\n",
       " 'seehof',\n",
       " 'seh',\n",
       " 'seh ',\n",
       " 'sei',\n",
       " 'seit',\n",
       " 'seit ',\n",
       " 'setz',\n",
       " 'setzt',\n",
       " 'sich',\n",
       " 'sich ',\n",
       " 'sieht',\n",
       " 'sinn',\n",
       " 'situation',\n",
       " 'sitz',\n",
       " 'sitzung',\n",
       " 'sitzungswoch',\n",
       " 'sofort',\n",
       " 'sogar',\n",
       " 'solidaritat',\n",
       " 'soll',\n",
       " 'sollt',\n",
       " 'sonntag',\n",
       " 'sorg',\n",
       " 'sozial',\n",
       " 'spannend',\n",
       " 'spass',\n",
       " 'spat',\n",
       " 'spd',\n",
       " 'spd ',\n",
       " 'spdbt',\n",
       " 'spdbt ',\n",
       " 'spdde',\n",
       " 'spdde ',\n",
       " 'spiegelonlin',\n",
       " 'spiel',\n",
       " 'sprach',\n",
       " 'sprech',\n",
       " 'sprech ',\n",
       " 'spricht',\n",
       " 'staat',\n",
       " 'stadt',\n",
       " 'stand',\n",
       " 'stark',\n",
       " 'stark ',\n",
       " 'start',\n",
       " 'statt',\n",
       " 'statt ',\n",
       " 'steh',\n",
       " 'steht',\n",
       " 'steht ',\n",
       " 'stell',\n",
       " 'stell ',\n",
       " 'stellt',\n",
       " 'steu',\n",
       " 'stimm',\n",
       " 'stimm ',\n",
       " 'stimmt',\n",
       " 'stimmt ',\n",
       " 'stimmung',\n",
       " 'strass',\n",
       " 'studi',\n",
       " 'stund',\n",
       " 'stuttgart',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'syri',\n",
       " 'sz',\n",
       " 'tag',\n",
       " 'tag ',\n",
       " 'tagesschau',\n",
       " 'tagesspiegel',\n",
       " 'tat',\n",
       " 'tatsach',\n",
       " 'tb',\n",
       " 'team',\n",
       " 'teil',\n",
       " 'teil ',\n",
       " 'termin',\n",
       " 'that',\n",
       " 'the',\n",
       " 'them',\n",
       " 'thema',\n",
       " 'thema ',\n",
       " 'this',\n",
       " 'thuring',\n",
       " 'tl',\n",
       " 'to',\n",
       " 'toll',\n",
       " 'treff',\n",
       " 'trifft',\n",
       " 'trotz',\n",
       " 'trotzd',\n",
       " 'trump',\n",
       " 'tun',\n",
       " 'tun ',\n",
       " 'turkei',\n",
       " 'tut',\n",
       " 'tweet',\n",
       " 'twitt',\n",
       " 'twitt ',\n",
       " 'u',\n",
       " 'u ',\n",
       " 'u.a',\n",
       " 'u.a ',\n",
       " 'ubrig',\n",
       " 'uhr',\n",
       " 'uhr ',\n",
       " 'ukrain',\n",
       " 'union',\n",
       " 'uns',\n",
       " 'unternehm',\n",
       " 'unterschied',\n",
       " 'unterstutz',\n",
       " 'unterstutz ',\n",
       " 'unterstutzt',\n",
       " 'unterweg',\n",
       " 'usa',\n",
       " 'v',\n",
       " 'veranstalt',\n",
       " 'verantwort',\n",
       " 'verantwort ',\n",
       " 'verbot',\n",
       " 'verdient',\n",
       " 'vergess',\n",
       " 'verstand',\n",
       " 'verstand ',\n",
       " 'versteh',\n",
       " 'versteh ',\n",
       " 'versuch',\n",
       " 'vertrau',\n",
       " 'vertret',\n",
       " 'via',\n",
       " 'via ',\n",
       " 'video',\n",
       " 'viel',\n",
       " 'viel dank',\n",
       " 'vielleicht',\n",
       " 'volk',\n",
       " 'voll',\n",
       " 'vollig',\n",
       " 'vorbei',\n",
       " 'vorschlag',\n",
       " 'vorsitz',\n",
       " 'wahl',\n",
       " 'wahl ',\n",
       " 'wahlkampf',\n",
       " 'wahlkampf ',\n",
       " 'wahlkreis',\n",
       " 'wahlkreis ',\n",
       " 'wann',\n",
       " 'war',\n",
       " 'war ',\n",
       " 'wart',\n",
       " 'warum',\n",
       " 'warum ',\n",
       " 'we',\n",
       " 'wed',\n",
       " 'weg',\n",
       " 'weiss',\n",
       " 'weiss ',\n",
       " 'weit',\n",
       " 'welt',\n",
       " 'welt ',\n",
       " 'wenig',\n",
       " 'wenig ',\n",
       " 'wer',\n",
       " 'wer ',\n",
       " 'wert',\n",
       " 'wichtig',\n",
       " 'wichtig ',\n",
       " 'willkomm',\n",
       " 'wirklich',\n",
       " 'wirtschaft',\n",
       " 'wiss',\n",
       " 'wiss ',\n",
       " 'wissenschaft',\n",
       " 'with',\n",
       " 'woch',\n",
       " 'woch ',\n",
       " 'wochen',\n",
       " 'wohl',\n",
       " 'word',\n",
       " 'wort',\n",
       " 'wort ',\n",
       " 'wunsch',\n",
       " 'wurd',\n",
       " 'wurd ',\n",
       " 'you',\n",
       " 'z.b',\n",
       " 'z.b ',\n",
       " 'zahl',\n",
       " 'zdf',\n",
       " 'zeich',\n",
       " 'zeig',\n",
       " 'zeig ',\n",
       " 'zeigt',\n",
       " 'zeigt ',\n",
       " 'zeit',\n",
       " 'zeit ',\n",
       " 'ziel',\n",
       " 'zug',\n",
       " 'zukunft',\n",
       " 'zukunft ',\n",
       " 'zuruck',\n",
       " 'zuruck ',\n",
       " 'zusamm',\n",
       " 'zusamm ',\n",
       " 'zusammenarbeit',\n",
       " 'zustand',\n",
       " 'zustimm',\n",
       " 'zwei',\n",
       " 'zweit',\n",
       " '\\u2066',\n",
       " '\\u2066 ',\n",
       " 'üá©üá™',\n",
       " 'üëç',\n",
       " 'üòâ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors=vectorizer_tf.transform(X_test.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84684, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 646. MiB for an array with shape (84684, 1000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-9530565c4d5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectorizer_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 646. MiB for an array with shape (84684, 1000) and data type float64"
     ]
    }
   ],
   "source": [
    "train_df=pd.DataFrame(train_vectors.toarray(), columns=vectorizer_tf.get_feature_names())\n",
    "train_df=pd.concat([train_df,X_train[\"target\"].reset_index(drop=True)], axis=1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.DataFrame(test_vectors.toarray(), columns=vectorizer_tf.get_feature_names())\n",
    "test_df=pd.concat([test_df,X_test[\"target\"].reset_index(drop=True)], axis=1)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import win_unicode_console\n",
    "win_unicode_console.enable()\n",
    "\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "if sys.stdout.encoding != 'cp850':\n",
    "  sys.stdout = codecs.getwriter('cp850')(sys.stdout, 'strict')\n",
    "if sys.stderr.encoding != 'cp850':\n",
    "  sys.stderr = codecs.getwriter('cp850')(sys.stderr, 'strict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h2o_train_df=h2o.H2OFrame(train_df)\n",
    "h2o_test_df=h2o.H2OFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o_train_df.types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o_train_df[\"target\"]=h2o_train_df[\"target\"].asfactor()\n",
    "h2o_test_df[\"target\"]=h2o_test_df[\"target\"].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import ML LIB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#import Evaluation Metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Runjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#get stemmer, stop_words\n",
    "stemmer = nltk.stem.SnowballStemmer(\"german\")\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words=set(nltk.corpus.stopwords.words(\"german\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Partei</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "      <td>@green_clarity99 Danke dir!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "      <td>Die Industrie ist das R√ºckgrat unseres Wohlsta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "      <td>@peter_simone @RenateKuenast Herzlichen Dank l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "      <td>Ebenfalls herzlichen Gl√ºckwunsch, liebe @Renat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "      <td>@BernhardPoetter treffend: ‚Äûüåç CO2-R√ºckgang 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806089</th>\n",
       "      <td>CDU</td>\n",
       "      <td>Mit bis zu 20 Mio ‚Ç¨ wird das ‚Å¶@BMBF_Bund‚Å© k√ºnf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806090</th>\n",
       "      <td>CDU</td>\n",
       "      <td>Die Simulation ist neben Theorie und Experimen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806091</th>\n",
       "      <td>CDU</td>\n",
       "      <td>Schule darf keine ‚Äûdigitalen Analphabeten‚Äú erz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806092</th>\n",
       "      <td>CDU</td>\n",
       "      <td>@StefanKaufmann @helmholtz_de üëç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806093</th>\n",
       "      <td>CDU</td>\n",
       "      <td>Bei der Springer Medizin Gala 2018 darf ich f√º...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1806094 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Partei  \\\n",
       "0        B√ºndnis 90/Die Gr√ºnen   \n",
       "1        B√ºndnis 90/Die Gr√ºnen   \n",
       "2        B√ºndnis 90/Die Gr√ºnen   \n",
       "3        B√ºndnis 90/Die Gr√ºnen   \n",
       "4        B√ºndnis 90/Die Gr√ºnen   \n",
       "...                        ...   \n",
       "1806089                    CDU   \n",
       "1806090                    CDU   \n",
       "1806091                    CDU   \n",
       "1806092                    CDU   \n",
       "1806093                    CDU   \n",
       "\n",
       "                                                     Tweet  \n",
       "0                              @green_clarity99 Danke dir!  \n",
       "1        Die Industrie ist das R√ºckgrat unseres Wohlsta...  \n",
       "2        @peter_simone @RenateKuenast Herzlichen Dank l...  \n",
       "3        Ebenfalls herzlichen Gl√ºckwunsch, liebe @Renat...  \n",
       "4        @BernhardPoetter treffend: ‚Äûüåç CO2-R√ºckgang 202...  \n",
       "...                                                    ...  \n",
       "1806089  Mit bis zu 20 Mio ‚Ç¨ wird das ‚Å¶@BMBF_Bund‚Å© k√ºnf...  \n",
       "1806090  Die Simulation ist neben Theorie und Experimen...  \n",
       "1806091  Schule darf keine ‚Äûdigitalen Analphabeten‚Äú erz...  \n",
       "1806092                    @StefanKaufmann @helmholtz_de üëç  \n",
       "1806093  Bei der Springer Medizin Gala 2018 darf ich f√º...  \n",
       "\n",
       "[1806094 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build DataFrame\n",
    "pnt_df2 = pd.DataFrame(partyname_n_tweets)\n",
    "pnt_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(pnt_df2[\"Tweet\"], pnt_df2.Partei, test_size = 0.3, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802192     Ja, es d√ºrfen alle kommen #bewegungjetzt und N...\n",
       "1573011    Volle Zustimmung zu Christoph von Marschall im...\n",
       "1020826    So, genau zu #MarkusKrebs auf #RTL wieder p√ºnk...\n",
       "52074      Parlamentarische Mehrheit im Europarat f√ºr pal...\n",
       "639127     Datenabgleich etc. scheitern bestimmt nicht an...\n",
       "                                 ...                        \n",
       "102486     @HanseNico @penzium Bin in Wrist, ist vermutli...\n",
       "135892     @gudruncita @mrohrlack @h_beppo @dadres @Ludwi...\n",
       "534484     Meine Bundestagsrede zur #Frauenquote als Film...\n",
       "654188     @LarsWendland @jorg_radek @GdPPresse Nach unse...\n",
       "1519444    @spektrallinie \"In\" sagt Volkmar Sigusch. Verm...\n",
       "Name: cleaned, Length: 1264265, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B√ºndnis 90/Die Gr√ºnen    433846\n",
       "SPD                      354097\n",
       "CDU                      336243\n",
       "Die Linke                326787\n",
       "FDP                      182542\n",
       "AfD                      102382\n",
       "CSU                       70197\n",
       "Name: Partei, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnt_df2[\"Partei\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B√ºndnis 90/Die Gr√ºnen    303776\n",
       "SPD                      247627\n",
       "CDU                      235729\n",
       "Die Linke                228384\n",
       "FDP                      128032\n",
       "AfD                       71564\n",
       "CSU                       49153\n",
       "Name: Partei, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B√ºndnis 90/Die Gr√ºnen    130070\n",
       "SPD                      106470\n",
       "CDU                      100514\n",
       "Die Linke                 98403\n",
       "FDP                       54510\n",
       "AfD                       30818\n",
       "CSU                       21044\n",
       "Name: Partei, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write our own tokenizer function\n",
    "def tokenize(text):\n",
    "    tokens = [word for word in nltk.word_tokenize(text)] #if (len(word) > 3 and len(word.strip(\"Xx/\")) > 2 and len(re.sub(\"\\d+\", \"\", word.strip(\"Xx/\")))]\n",
    "    tokens = map(str.lower, tokens)\n",
    "    tokens = [word.strip(\" #|‚Ç¨!?,.:;'`‚Äò‚Äô‚Äú‚Äù‚Äû@[]()=*+-‚Äì%/0123456789& \") for word in tokens]\n",
    "    #tokens = [word for word in tokens if not word in emoji.UNICODE_EMOJI]\n",
    "    #tokens = [word.strip() for word in tokens]\n",
    "    stems = [item for item in tokens if (item not in stop_words)]\n",
    "    #stems = [stemmer.stem(item) for item in tokens if (item not in stop_words)]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram_range = how many words to consider, 1, 2 --> look at every seperate word, and a pair of 2 words: not good --> bad\n",
    "pipeline = Pipeline([(\"vect\", TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 2), stop_words=None, sublinear_tf=True, max_df=0.75, lowercase = False)), \n",
    "                     (\"chi\", SelectKBest(chi2, k=10000)),\n",
    "                     (\"clf\", LinearSVC(C=1.0, penalty=\"l1\", max_iter=6000, dual=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Runjie\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = model.named_steps[\"vect\"]\n",
    "chi = model.named_steps[\"chi\"]\n",
    "clf = model.named_steps[\"clf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "feature_names = [feature_names[i] for i in chi.get_support(indices = True)]\n",
    "feature_names = np.asarray(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 30 keywords per class:\n",
      "AfD: merkeldeutschland bessermenschen altparteienstaat repswalwell zwangsgeb√ºhren  antifaterrorists j√ºrgen braun gr√ºnlinken afdbayern merkelland kestner einheitsmedien  gezabschaffen mrosek intelligenzgibtsnichtimbioladen bernhard https nurnochafd  toterjournalismus ramel√ºringen afdw√§hlen gr√ºnlinke afdimbundestag afdüë• afdimbundestag‚Å©  afdwirkt  traudichdeutschland klimawahn buntland ardzdf altparteien\n",
      "\n",
      "\n",
      "B√ºndnis 90/Die Gr√ºnen:  enveryucel  lavonrw rio http  darumgruen erststimmemutlu ms https  selcukrsirin mutlu hocam üåªüåªüåª gerhard schick gruenenrw uetd tabeaf√§hrtbahn garantierente queer_de zukunftentscheidetsichhier liz_de marc_lueger klimakrise bdk gruenemitte albtour sw http  gruenebundestag greenrunners tt ssr mutludirekt b√º\n",
      "\n",
      "\n",
      "CDU: d√ºlken kreisreform neustaat klimke lindow hdh cducsubt‚Å© moltkezitate cdu-kreisverband kommentar woche viersen lb bremen-nord stgt cda neuruppin  euerpaul willicher hohensteinernstthal sachsenring aicgs cducsubt  teamr√∂ttgen jetztvoran stephanuskreis willich neersen rhein-wied windhagen steineke\n",
      "\n",
      "\n",
      "CSU:  dibncc  spiegelplag csuklpt  ü§∑üèª‚Äç‚ôÇÔ∏è schweinfurt-land csu-wahlprogramm erlanger anja https werneck w√∂hrlwideweb fcbayern_news  jazubayern  dgl eppdublin ellikoestinger  volkach csu-europagruppe  t.co/h63dbe12cp  t.co/r5s4bwfd schwebheim ettling wallersdorfer lounge the wallersdorf schweinfurter  t.co/znuqzg4lbv  aspdercsu woÃàhrl  bwk dhdl\n",
      "\n",
      "\n",
      "Die Linke: milliard√§re nachdenkseiten geretsried bayreuth-forchheim guerillaknitting  dielinke_nrw  cdumandatabnehmen krls goldkronacher f√ºrstenberg pirateparty linkeept rosaluxstiftung guerillastricken searchers cccamp dprinting f√ºrstenberg/havel  nog dtag f√ºhrt gespr√§ch linksfraktion danke rt  mahe ‚ù§Ô∏è‚úä‚ù§Ô∏è koschyk achim kessler masi ü§∑üèº goldkronach\n",
      "\n",
      "\n",
      "FDP:  fabiangrischkat heubisch freien demokraten ^df jci icann hagen reinhold  smrc  weltbestebildung https freie demokraten mc_disrupt  fdplgby tjm christian jung tcd  mahlo  prometheusinst co2-limit luksic winfriedfelser feynmansmethod  notitsen liberale_news masz team beer cl tb fdpbt cj\n",
      "\n",
      "\n",
      "SPD: w√ºlfrath ratingen neu blog  depesche welschbach originalchemnitz  spdbawue  malteengeler asf spd-ortsverein  harlemdesir  europeunited kiezregiert feinen tag gabi weber thews  strenz  socpace doroistda  europaistdieantwort retweeted bissigerfisch heringen spdbt rimkus spd-bundestagsfraktion lem me yfj  medusa schaut webseite\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#target_names = [\"B√ºndnis 90/Die Gr√ºnen\", \"SPD\", \"CDU\", \"Die Linke\", \"FDP\", \"AfD\", \"CSU\"]\n",
    "target_names = clf.classes_\n",
    "print(\"top 30 keywords per class:\")\n",
    "for i, label in enumerate(target_names):\n",
    "    top30 = np.argsort(clf.coef_[i])[-30:]\n",
    "    print(\"%s: %s\" % (label, \" \".join(feature_names[top30])))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******SVM classification report (full tweets, all data)******\n",
      "accuracy: 0.5215501569683424\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "                  AfD     0.5726    0.6074    0.5895     30818\n",
      "B√ºndnis 90/Die Gr√ºnen     0.4694    0.6365    0.5403    130070\n",
      "                  CDU     0.5090    0.4832    0.4957    100514\n",
      "                  CSU     0.6668    0.2983    0.4122     21044\n",
      "            Die Linke     0.5818    0.5193    0.5488     98403\n",
      "                  FDP     0.7248    0.4310    0.5406     54510\n",
      "                  SPD     0.4796    0.4851    0.4823    106470\n",
      "\n",
      "             accuracy                         0.5216    541829\n",
      "            macro avg     0.5720    0.4944    0.5156    541829\n",
      "         weighted avg     0.5384    0.5216    0.5200    541829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"******SVM classification report (full tweets, all data)******\")\n",
    "print(\"accuracy:\", accuracy_score(y_test, prediction))\n",
    "print(classification_report(y_test, prediction, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********SVM confusion matrix (full tweets, all data)*******\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow0_col0,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow0_col1,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow0_col2,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow0_col3,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow0_col4,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow0_col5,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow0_col6,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow1_col0,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow1_col1,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow1_col2,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow1_col3,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow1_col4,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow1_col5,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow1_col6,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow2_col0,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow2_col1,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow2_col2,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow2_col3,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow2_col4,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow2_col5,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow2_col6,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow3_col0,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow3_col1,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow3_col2,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow3_col3,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow3_col4,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow3_col5,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow3_col6,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow4_col0,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow4_col1,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow4_col2,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow4_col3,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow4_col4,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow4_col5,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow4_col6,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow5_col0,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow5_col1,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow5_col2,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow5_col3,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow5_col4,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow5_col5,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow5_col6,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow6_col0,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow6_col1,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow6_col2,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow6_col3,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow6_col4,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow6_col5,#T_d966197c_bfcd_11eb_ae13_305a3a083b3frow6_col6{\n",
       "            text-align:  center;\n",
       "        }</style><table id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3f\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >pred:AfD</th>        <th class=\"col_heading level0 col1\" >pred:B√ºndnis 90/Die Gr√ºnen</th>        <th class=\"col_heading level0 col2\" >pred:CDU</th>        <th class=\"col_heading level0 col3\" >pred:CSU</th>        <th class=\"col_heading level0 col4\" >pred:Die Linke</th>        <th class=\"col_heading level0 col5\" >pred:FDP</th>        <th class=\"col_heading level0 col6\" >pred:SPD</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3flevel0_row0\" class=\"row_heading level0 row0\" >true:AfD</th>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow0_col0\" class=\"data row0 col0\" >18719</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow0_col1\" class=\"data row0 col1\" >3252</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow0_col2\" class=\"data row0 col2\" >2048</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow0_col3\" class=\"data row0 col3\" >65</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow0_col4\" class=\"data row0 col4\" >2209</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow0_col5\" class=\"data row0 col5\" >610</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow0_col6\" class=\"data row0 col6\" >3915</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3flevel0_row1\" class=\"row_heading level0 row1\" >true:B√ºndnis 90/Die Gr√ºnen</th>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow1_col0\" class=\"data row1 col0\" >2685</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow1_col1\" class=\"data row1 col1\" >82791</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow1_col2\" class=\"data row1 col2\" >12174</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow1_col3\" class=\"data row1 col3\" >908</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow1_col4\" class=\"data row1 col4\" >12682</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow1_col5\" class=\"data row1 col5\" >2556</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow1_col6\" class=\"data row1 col6\" >16274</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3flevel0_row2\" class=\"row_heading level0 row2\" >true:CDU</th>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow2_col0\" class=\"data row2 col0\" >2738</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow2_col1\" class=\"data row2 col1\" >23890</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow2_col2\" class=\"data row2 col2\" >48564</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow2_col3\" class=\"data row2 col3\" >873</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow2_col4\" class=\"data row2 col4\" >7414</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow2_col5\" class=\"data row2 col5\" >1656</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow2_col6\" class=\"data row2 col6\" >15379</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3flevel0_row3\" class=\"row_heading level0 row3\" >true:CSU</th>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow3_col0\" class=\"data row3 col0\" >489</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow3_col1\" class=\"data row3 col1\" >5704</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow3_col2\" class=\"data row3 col2\" >3534</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow3_col3\" class=\"data row3 col3\" >6277</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow3_col4\" class=\"data row3 col4\" >1339</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow3_col5\" class=\"data row3 col5\" >1037</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow3_col6\" class=\"data row3 col6\" >2664</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3flevel0_row4\" class=\"row_heading level0 row4\" >true:Die Linke</th>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow4_col0\" class=\"data row4 col0\" >2941</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow4_col1\" class=\"data row4 col1\" >23016</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow4_col2\" class=\"data row4 col2\" >8730</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow4_col3\" class=\"data row4 col3\" >327</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow4_col4\" class=\"data row4 col4\" >51101</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow4_col5\" class=\"data row4 col5\" >1076</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow4_col6\" class=\"data row4 col6\" >11212</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3flevel0_row5\" class=\"row_heading level0 row5\" >true:FDP</th>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow5_col0\" class=\"data row5 col0\" >1827</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow5_col1\" class=\"data row5 col1\" >12074</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow5_col2\" class=\"data row5 col2\" >6003</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow5_col3\" class=\"data row5 col3\" >414</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow5_col4\" class=\"data row5 col4\" >4095</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow5_col5\" class=\"data row5 col5\" >23494</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow5_col6\" class=\"data row5 col6\" >6603</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3flevel0_row6\" class=\"row_heading level0 row6\" >true:SPD</th>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow6_col0\" class=\"data row6 col0\" >3295</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow6_col1\" class=\"data row6 col1\" >25642</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow6_col2\" class=\"data row6 col2\" >14359</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow6_col3\" class=\"data row6 col3\" >549</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow6_col4\" class=\"data row6 col4\" >8993</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow6_col5\" class=\"data row6 col5\" >1987</td>\n",
       "                        <td id=\"T_d966197c_bfcd_11eb_ae13_305a3a083b3frow6_col6\" class=\"data row6 col6\" >51645</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e4a6509e20>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"********SVM confusion matrix (full tweets, all data)*******\")\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_test, prediction, labels=target_names),\n",
    "    index = [\"true:{:}\".format(x) for x in target_names],\n",
    "    columns = [\"pred:{:}\".format(x) for x in target_names]\n",
    ")\n",
    "cm.style.set_properties(**{'text-align': 'center'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram_range = how many words to consider, 1, 2 --> look at every seperate word, and a pair of 2 words: not good --> bad\n",
    "pipeline = Pipeline([(\"vect\", TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 2), stop_words=None, sublinear_tf=True, max_df=0.75, lowercase = False)), \n",
    "                     #(\"chi\", SelectKBest(chi2, k=10000)),\n",
    "                     (\"clf\", DecisionTreeClassifier(random_state = 111))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-6c54b1b8e552>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m    329\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    332\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    293\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \"\"\"\n\u001b[0;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1841\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1110\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1113\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = model.named_steps[\"vect\"]\n",
    "#chi = model.named_steps[\"chi\"]\n",
    "clf = model.named_steps[\"clf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.feature_importances_[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"******DecisionTree classification report (full tweets, all data)******\")\n",
    "print(\"accuracy:\", accuracy_score(y_test, prediction))\n",
    "print(classification_report(y_test, prediction, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"********DecisionTree confusion matrix (full tweets, all data)*******\")\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_test, prediction, labels=target_names),\n",
    "    index = [\"true:{:}\".format(x) for x in target_names],\n",
    "    columns = [\"pred:{:}\".format(x) for x in target_names]\n",
    ")\n",
    "cm.style.set_properties(**{'text-align': 'center'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram_range = how many words to consider, 1, 2 --> look at every seperate word, and a pair of 2 words: not good --> bad\n",
    "pipeline = Pipeline([(\"vect\", TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 2), stop_words=None, sublinear_tf=True, max_df=0.75, lowercase = False)), \n",
    "                     (\"chi\", SelectKBest(chi2, k=10000)),\n",
    "                     (\"clf\", RandomForestClassifier(random_state = 111))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = model.named_steps[\"vect\"]\n",
    "chi = model.named_steps[\"chi\"]\n",
    "clf = model.named_steps[\"clf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "feature_names = [feature_names[i] for i in chi.get_support(indices = True)]\n",
    "feature_names = np.asarray(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5324056346290156\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "                  AfD     0.4551    0.3262    0.3800      4409\n",
      "B√ºndnis 90/Die Gr√ºnen     0.4844    0.3987    0.4374     15176\n",
      "                  CDU     0.5574    0.4867    0.5197     22299\n",
      "                  CSU     0.5602    0.3600    0.4383      5159\n",
      "            Die Linke     0.6367    0.6492    0.6429     39751\n",
      "                  FDP     0.4757    0.3066    0.3729      7596\n",
      "                  SPD     0.4376    0.6000    0.5061     28351\n",
      "\n",
      "             accuracy                         0.5324    122741\n",
      "            macro avg     0.5153    0.4468    0.4710    122741\n",
      "         weighted avg     0.5378    0.5324    0.5288    122741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, prediction))\n",
    "print(classification_report(y_test, prediction, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, prediction)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram_range = how many words to consider, 1, 2 --> look at every seperate word, and a pair of 2 words: not good --> bad\n",
    "pipeline = Pipeline([(\"vect\", TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 2), stop_words=None, sublinear_tf=True, max_df=0.75, lowercase = False)), \n",
    "                     #(\"chi\", SelectKBest(chi2, k=10000)),\n",
    "                     (\"clf\", GradientBoostingClassifier(random_state = 111))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-6c54b1b8e552>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m    329\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    332\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    293\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \"\"\"\n\u001b[0;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1841\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m             \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m             \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = model.named_steps[\"vect\"]\n",
    "#chi = model.named_steps[\"chi\"]\n",
    "clf = model.named_steps[\"clf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, prediction))\n",
    "print(classification_report(y_test, prediction, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, prediction)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram_range = how many words to consider, 1, 2 --> look at every seperate word, and a pair of 2 words: not good --> bad\n",
    "pipeline = Pipeline([(\"vect\", TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 2), stop_words=None, sublinear_tf=True, max_df=0.75, lowercase = False)), \n",
    "                     (\"chi\", SelectKBest(chi2, k=10000)),\n",
    "                     (\"clf\", GradientBoostingClassifier(random_state = 111))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = model.named_steps[\"vect\"]\n",
    "chi = model.named_steps[\"chi\"]\n",
    "clf = model.named_steps[\"clf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4868218443714814\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "                  AfD     0.7035    0.3035    0.4240      4409\n",
      "B√ºndnis 90/Die Gr√ºnen     0.8557    0.2044    0.3300     15176\n",
      "                  CDU     0.7738    0.2974    0.4297     22299\n",
      "                  CSU     0.7589    0.3125    0.4427      5159\n",
      "            Die Linke     0.4199    0.8940    0.5714     39751\n",
      "                  FDP     0.7320    0.2075    0.3233      7596\n",
      "                  SPD     0.5044    0.3512    0.4141     28351\n",
      "\n",
      "             accuracy                         0.4868    122741\n",
      "            macro avg     0.6783    0.3672    0.4193    122741\n",
      "         weighted avg     0.6014    0.4868    0.4534    122741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, prediction))\n",
    "print(classification_report(y_test, prediction, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1338    19   116     7  1996    37   896]\n",
      " [   76  3102   355    59 10014   118  1452]\n",
      " [  142   134  6632    84 11922    86  3299]\n",
      " [    8    16    96  1612  2935    17   475]\n",
      " [  163   144   522   142 35536   138  3106]\n",
      " [   54    71   117    97  5127  1576   554]\n",
      " [  121   139   733   123 17097   181  9957]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, prediction)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram_range = how many words to consider, 1, 2 --> look at every seperate word, and a pair of 2 words: not good --> bad\n",
    "pipeline = Pipeline([(\"vect\", TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 2), stop_words=None, sublinear_tf=True, max_df=0.75, lowercase = False)), \n",
    "                     (\"chi\", SelectKBest(chi2, k=10000)),\n",
    "                     (\"clf\", MLPClassifier(hidden_layer_sizes=(7,7), random_state = 111))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = model.named_steps[\"vect\"]\n",
    "chi = model.named_steps[\"chi\"]\n",
    "clf = model.named_steps[\"clf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "feature_names = [feature_names[i] for i in chi.get_support(indices = True)]\n",
    "feature_names = np.asarray(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6169658060468792\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "                  AfD     0.4159    0.4625    0.4379      4409\n",
      "B√ºndnis 90/Die Gr√ºnen     0.6867    0.4918    0.5731     15176\n",
      "                  CDU     0.6433    0.5779    0.6089     22299\n",
      "                  CSU     0.7732    0.4811    0.5931      5159\n",
      "            Die Linke     0.6780    0.7435    0.7093     39751\n",
      "                  FDP     0.6658    0.4406    0.5303      7596\n",
      "                  SPD     0.5113    0.6333    0.5658     28351\n",
      "\n",
      "             accuracy                         0.6170    122741\n",
      "            macro avg     0.6249    0.5472    0.5741    122741\n",
      "         weighted avg     0.6281    0.6170    0.6154    122741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, prediction))\n",
    "print(classification_report(y_test, prediction, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2039    82   435    37   729   140   947]\n",
      " [  219  7464  1129   103  2923   334  3004]\n",
      " [  540   721 12887   190  2864   245  4852]\n",
      " [  153   206   577  2482   741   136   864]\n",
      " [  722   977  1840    85 29554   463  6110]\n",
      " [  226   431   501   143  1567  3347  1381]\n",
      " [ 1004   989  2663   170  5209   362 17954]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, prediction)\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
